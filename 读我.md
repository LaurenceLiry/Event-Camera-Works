***本文档记录事件相机学习过程中阅读相关文献的信息及个人体会***

# 文献目录

| 文献名 | 发表时间 | 刊物 | 简介 |
| - | - | - | - |
| Fast, Accurate Thin-Structure Obstacle Detection for Autonomous Mobile Robots | 2017 | CVPR | 传统相机做的细小障碍物检测|
| :star: Semi-Dense 3D Reconstruction with a Stereo Event Camera | 2018 | ECCV | 事件相机3D重建 |
| :star: Event-Based Line Fitting and Segment Detection Using a Neuromorphic Visual Sensor | 2019 | TNNLS | 顶刊事件相机针对线的检测 |
| Event Guided Depth Sensing | 2021 | 3DV | 基于事件相机的深度估计方法 |
| :star: Powerline Tracking with Event Cameras | 2021 | IROS | 事件相机做的无人机电力线巡检 |
| Dense Voxel 3D Reconstruction Using a Monocular Event Camera | 2023 | ICVR | 基于时间相机的3D体素三维重建 |
| :star: E-NeRF: Neural Radiance Fields from a Moving Event Camera | 2023 | RA-L | 事件相机NeRF/通用3D重建 |
| :star: Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor | 2024 | Conference on Robot Learning | 基于事件相机的无人机避障 |
| :star: Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera | 2025 | ICCV | 基于事件相机的线的检测与跟踪 |
| :star: Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety | 2025 | 预印本 | 无人机事件相机极细装障碍物检测 |

[ETH维护的事件相机资料汇总](https://github.com/uzh-rpg/event-based_vision_resources)

---

# 刊物

**RA-L *(以下内容摘自[IEEE官网](https://www.ieee-ras.org/publications/ra-l/information-for-authors-ra-l))***

**注意：南理工不认RA-L**

稿件长度为**六页**，采用双栏IEEE格式，包括图表和参考文献，最多可额外增加两页（需另付页数费用），**总页数不超过八页**。欢迎包含多媒体材料，例如视频、数据存储库、代码源等。其他补充材料，例如附录，不得超过八页的限制。

RA-L是一份纯电子版的混合型开放获取期刊。作者提交的最终版本将迅速发布在IEEE XPlore上，随后不久将被编辑后的​​XML标签版本替换。**RA-L全年接受投稿**。稿件将在收到后**六个月内发表**，或在同行评审过程中被拒稿。RA-L的论文在接收后会很快在线发表，因此作者应在投稿前完成所有知识产权/专利相关手续。

**IROS *(IROS历届召开时间地点[IEEE官网](https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros/past-and-future-venues?utm_source=chatgpt.com))***

**IROS 2026 Pittsburgh (PA), USA [相关信息(其中的时间不可信)](https://conferenceinc.net/post/iros-2026/)**

**IEEE模板**：[IEEE模板点这里](https://www.ieee.org/conferences/publishing/templates)

---

# 基本原理

1. **事件相机**
   
传统相机的缺点

- 延迟较高：传统相机受制于帧率的限制，往往只有数十Hz的帧率，即便高速相机（比如帧率几百甚至达到kHz）也具有较高的延迟（1ms）。
- 动态性能差：传统相机需要光子积累来达到曝光，这段时间内目标物体移动会导致画面模糊
- 动态范围低：光线差或光线极强时，画面信息会被破坏

事件相机的原理

当某个像素的亮度变化累计达到一定阈值后，输出一个**事件**。
- 亮度变化：说明事件相机的输出和变化有关，而与亮度的绝对值没有关系；
- 阈值：当亮度变化达到一定程度时，将输出数据，这个阈值是相机的固有参数。
- 事件：事件具有三要素：时间戳、像素坐标与极性。一个事件表达的是**在什么时间，哪个像素点，发生了亮度的增加或减小**。

当场景中由物体运动或光照改变造成大量像素变化时，会产生一系列的事件，这些事件以事件流(Events stream)方式输出。事件流的数据量远小于传统相机传输的数据，且事件流没有最小时间单位，所以不像传统相机定时输出数据，具有低延迟特性。

![事件相机vs传统相机](事件相机原理.gif)

总之一句话概括：只要亮度一有变化就会输出，且仅输出变化的数据占用了很小的带宽，同时由于事件相机更擅长捕捉亮度变化，所以在较暗和强光场景下也能输出有效数据。

---

# 文献阅读

## 1. Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety

传统方法（如运动估计技术[1]）在应对这些障碍物时表现不佳，因为这些细线具有弱视觉显著性且易受运动伪影影响

事件相机研究现状：将事件相机用于无人机的相关研究已用于障碍规避[3]、空中物体定位[4,5]及去噪[6]等任务

基于这一潜力，我们利用事件相机的独特能力，将细线结构转换为可检测的时间表面，从而为感知极细障碍物提供极具前景的解决方案。

本文解决的两个问题：

- Disambiguating from Background Edges（从背景边缘中区分障碍）
- Managing the Data Deluge（应对巨大数据流量）

提出了Skyshield，一个轻量级的事件驱动框架，实现实时亚毫米级细线障碍检测。

文章提出的结构由两个模块组成：事件数据预处理模块 + 轻量级的U形卷积网络LUnet(使用了自定义的Dice-Contour正则化损失)

**作者的2个发现**：
1. 在空间域中呈现为稀疏线状的细线障碍物，会被事件相机的运动捕获转换为时空域(x, y, t)中的稠密二维表面。这一扩展使得特征更加“庞大”，更易被识别。
2. 由于事件相机的极性特性，移动的细线障碍物会产生明显的双极性事件模式。其前缘产生一条正极性事件带，而后缘则产生一条相邻的负极性事件带。由于障碍物极细，这两条事件带在空间和时间上都非常接近。

**事件相机数据预处理（本文的方法）**：
第一步：使用时空对比滤波器（STC）去除原始事件流中的噪声，将事件率降低超过80%。
随后，将过滤后的异步事件转换为稠密、连续值的时间表面。其方法是将事件映射到每像素时间戳缓冲区 T(x,y)，并按指数衰减进行归一化。得到的浮点图像将事件聚合成紧凑表示，用作后续网络的输入。

**提取线特征的方法**：
使用LUnet的线特征提取，轻量级的LUnet网络用于高效提取细线特征，采用编码-解码结构，并通过跳跃连接捕捉空间和上下文信息。
为了利用细线障碍物的双边事件模式，作者提出了一种新的损失函数：Dice-Contour正则化损失。该损失由两部分组成：
- Dice Loss — 用于确保线条位置的准确性
- Contour Regularization Term — 用于鼓励输出更清晰、更细的预测

**实验基准**
作者将Skyshield与两种传统细线检测方法进行对比：
- Hough Transform（霍夫变换）
- LSD（Line Segment Detector）

使用的评估指标：
1. IoU（交并比）
2. Dice coefficient（用于 F1）
3. Inference Time（推理时间）

同时作者做了对比试验：激光雷达、RGB相机和事件相机检测同一个场景下的细线

```
[1] Dashuai Wang, Wei Li, X. Liu, Nan Li, and Chunlong Zhang. 2020. UAV environmental perception and autonomous obstacle avoidance: A deep learning and depth camera combined solution. Comput. Electron. Agric. 175 (2020), 105523. https://api.semanticscholar.org/CorpusID:225445299

[3] Jingao Xu, Danyang Li, Zheng Yang, Yishujie Zhao, Hao Cao, Yunhao Liu, and Longfei Shangguan. 2023. Taming Event Cameras with Bio-Inspired Architecture and Algorithm: A Case for Drone Obstacle Avoidance. In Processings of the 29th ACM MobiCom.

[4] Haoyang Wang, Jingao Xu, Xinyu Luo, Ting Zhang, Xuecheng Chen, Ruiyang Duan, Jialong Chen, Yunhao Liu, Jianfeng Zheng, Weijie Hong, et al. [n. d.]. mmE-Loc: Facilitating Accurate Drone Landing with UltraHigh-Frequency Localization. arXiv preprint arXiv:2507.09469 ([n. d.]).

[5] Xinyu Luo, Haoyang Wang, Ciyu Ruan, Chenxin Liang, Jingao Xu, and Xinlei Chen. 2024. EventTracker: 3D Localization and Tracking of High-Speed Object with Event and Depth Fusion. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking (Washington D.C., DC, USA) (ACM MobiCom ’24). Association for Computing Machinery, New York, NY, USA, 1974–1979. doi:10.1145/3636534.3694721

[6] Ciyu Ruan, Ruishan Guo, Zihang Gong, Jingao Xu, Wenhan Yang, and Xinlei Chen. 2025. PRE-Mamba: A 4D State Space Model for UltraHigh-Frequent Event Camera Deraining. arXiv:2505.05307 [cs.CV] https://arxiv.org/abs/2505.05307
```

## 2. Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera

[github主页](https://github.com/event-driven-robotics/RT-EvLDT)

仅使用事件相机的基于栅格分配的实时线段特征检测与跟踪

事件相机通过对边缘亮度变化进行异步响应，能够减少冗余数据，从而实现高效特征提取。

事件相机研究现状：
- 当前方法通常依赖额外的帧相机，或者在高事件速率下表现不佳。
- 近年来，与经典计算机视觉方法（如线段检测器LSD[16]）相比，基于深度学习的线段提取方法[17,30] 变得更加普遍。
- 然而，当相机或目标快速运动时，输入到网络的图像仍然会因运动模糊而出现质量退化，并且在高动态范围条件下会出现像素强度饱和的问题，并且通常需要GPU。

**遇到的瓶颈**：尽管事件-帧混合方法[7,32,34] 提升了性能，但它们对帧相机的依赖限制了全事件处理方法的潜力，尽管事件相机硬件已取得显著进步。
  
本文解决的一个问题：旨在仅使用现代高分辨率（即高事件速率）事件相机，实现实时线段检测与跟踪。

提出了基于栅格分配的pipeline（工作流）——“SCARF”(A Set of Centre Active Receptive Fields)，其包括：
1. 与速度无关的事件表示，
2. 基于拟合分数的线段检测，
3. 通过扰动端点实现的线段跟踪。

**提取线特征及跟踪方法**

根据文献[9]，基于事件的线段检测与跟踪方法通常分为三类：

| 方法 | 内容 | 局限性 |
| - | - | - |
| 基于霍夫变换的方法 | 基于霍夫变换的方法[10,18]通过在直线参数空间中寻找包含最多事件的直线来检测线段，但其计算量大，不适用于高事件速率场景 | 检测出的线段通常很短，并且对像素级噪声十分敏感 |
| 非参数方法 | 非参数方法[4]基于时间表面[3,28]的空间导数对像素进行聚类 | 检测出的线段通常很短，并且对像素级噪声十分敏感 |
| 时空方法 | 在足够短的时间窗口内，与某条线对齐的事件在𝑢,𝑣,𝑡空间中位于同一平面，因此可通过平面拟合来提取线段[10,11] | 能够得到更长、更稳定的线段，但其在细节丰富或曲线形状上的表现有限 |
| 深度学习方法 | 将事件与帧结合使用 | - |
| 对比度最大化方法 | 通过运动补偿实现事件图像锐化，从而得到精确的线段提取结果 | - |

**工作流程：SCARF->检测->跟踪**

这篇文章中对线段𝐿的定义：
1. 两个端点𝑞<sub>0</sub>=(𝑢<sub>𝑞0</sub>,𝑣<sub>𝑞0</sub>), 𝑞<sub>1</sub>=(𝑢<sub>𝑞1</sub>,𝑣<sub>𝑞1</sub>)
2. 一个在检测与跟踪中计算的拟合得分𝑓
3. 一个线段状态𝑠∈{NoDetect,Detected,ProhibitDetection,BadTrack,GoodTrack}
4. 拥有此线段的网格块坐标，称为Admin block（管理员块）(𝑟<sub>𝑢𝑏</sub>,𝑟<sub>𝑣𝑏</sub>)
5. 一个线段ID 𝑙<sub>𝑖𝑑</sub>

**对比方法**

![这篇文章的对比算法](对比方法.png)

- 作者与其他四种以前论文中的方法进行比较，分别是以下四种，代码已给出。

```
代码：
[ELiSeD](https://github.com/SensorsINI/jaer)
[Powerline](https://github.com/uzh-rpg/line_tracking_with_event_cameras)
[C2F-EFIO](https://github.com/choibottle/C2F-EFIO)
[FE-LSD](https://github.com/lh9171338/FE-LSD)
```

- 这五种pipelines使用**公开数据集进行评测：Event Camera Dataset** [29]（240×180），以及包含 RGB 帧和 IMU 任意六自由度运动的 MVSEC 数据集 [36]

[29] Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Delbruck, and Davide Scaramuzza. The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and slam. The International Journal of Robotics Research, 36(2):142–149, 2017.

[36]Alex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, and Kostas Daniilidis. The multivehicle stereo event camera dataset: An event camera dataset for 3d perception. IEEE Robotics and Automation Letters, 3(3):2032–2039, 2018.

- 为测试高事件速率下实时处理的极限，作者使用Prophesee EVK3（640×480）新录制的数据集上进行了评估，其中真实线段由人工标注生成。

> 在ELiSeD中，由于事件缺失，一些线段存在断裂。特别是在“circle_board”数据集中，由于垂直运动不足，水平线段经常被遗漏。Powerline也出现类似遗漏。

> 在“three_vertical_lines_fast”数据集中，更快的水平运动导致跟踪失败，从而降低Powerline的召回率。在复杂场景（如“dynamic_6dof”和“indoor_flying1”）中，Powerline无法准确检测与跟踪线段。

> C2F-EFIO对由运动速度变化造成的事件速率变化十分敏感，导致无法在整个视野中完整提取线段。

> FE-LSD提取的线段在准确性上优于我们的方法。然而FE-LSD对曲线形状需要额外微调。


**本文算法优点**

- 相比于其他四个pipeline的方法，本文的计算消耗较低，可以实时运行
- 其他四个pipeline捕捉到的线段或长或短、或缺失细节，得益于基于栅格的检测与跟踪框架，本文的方法相比C2F-EFIO和FE-LSD提取了更多线段，并且有更好的时间一致性。
- 实验有做参数敏感性评估

## 3. Powerline Tracking with Event Cameras

[github主页](https://github.com/uzh-rpg/line_tracking_with_event_cameras)

提出了一种利用事件相机来稳健跟踪电力线的方法

通过在时空信号中检测平面来识别事件流中的电力线，并随时间跟踪它们。该方法能够在机载设备上运行，并且可以在高达每秒32万事件的速率下实时检测多条独立的电力线

四旋翼无人机是一种能够快速部署且具成本效益的电力线巡检解决方案，使用四旋翼无人机可降低50%的成本[1]。整体而言，能源行业，特别是电力线巡检领域，对具备高度耦合的感知与控制的闭环方法存在根本需求[3]。稳健且持久的电力线跟踪对于感知驱动的规划至关重要[4],[5]。基于事件的线段跟踪难度促使先前研究[9]–[15]探索多种不同技术，但目标一致：**提高稳健性**。

作者将线段跟踪的稳健性定义为以下三种特性之组合：

1. 正确性（即能够只跟踪电力线本身的能力），
2. 实例性（即能够用唯一标识符持续跟踪同一条电力线的能力），
3. 持久性（即线段可被跟踪多长时间）。

**本文的方法在文献[15]的基础上扩展了新的处理模块——休眠机制。**

[15] L. Everding and J. Conradt, “Low-latency line tracking using eventbased dynamic vision sensors,” Frontiers in neurorobotics, vol. 12, p. 4, 2018.

**提取线特征及跟踪方法**

本文和上一篇文献提到的RELATED WORK几乎一样，也是如上表格中的三种研究方法，作者在分析后发现方法三（文献15）最适合较长直线的任务目标，故基于文献15的方法：

> 通过在事件的时空空间中，在短时间段内拟合平面来跟踪线段。当平面拟合满足某些条件时，即可检测到一条线

线段跟踪算法**四个主要步骤**
- 事件过滤：两个基于SAE(Surface of Active Events)的过滤器
- 加入线段：将过滤后符合条件的事件提升为Line，其余不超过阈值的事件归到簇中，还剩余的事件保持未分配状态，存储在SAE中
- 加入簇：将簇中满足条件的的事件提升为Line，创建簇的方法见*章节E*
- 直线提升：上述加入簇的具体算法，主要计算直线方向d、直线中点p、直线长度l

**G.休眠机制**

本文提出的休眠机制主要为了解决**方向改变的直线**的鲁棒性

方向改变：线段物理方向改变、线段在FOV中由于无人机的运动而改变

关键问题：
- 当方向突然改变时，触发的事件很少
- 此时估计的事件平面法向量不准确
- 若继续沿原方向更新直线，可能导致直线漂移并最终丢失

休眠机制作用：
- 当方向突然改变时，让直线暂停更新，而不是立即丢失
- 解决缓慢方向变化时事件稀少的问题
- 状态切换的依据：事件密度（作者自己定义的）

**H.系统维护**

1. 对initializing状态的线：

```
检查其初始化阶段是否结束；如果已结束，计算它们的connected length。
若连通长度超过70像素，则该线的状态从initializing变为active。
```

2. 对active状态的线：

```
更新拟合平面的估计；
检查线的长度是否过短。若线过短，则将其丢弃。
```

3. 对hibernated状态的线：

```
检查其休眠时间。如果休眠时间超过1秒，该线将被删除。
也检查最新事件的时间戳，它必须小于40ms。
最后，检查是否存在两条线可以合并。
```

**局限性**

- 本文的方法在目标线段做平移运动的时候表现最佳，而当目标线段快速旋转时，由于算法假设线在时空域中形成平面，会导致检测困难。

- 纹理丰富但无明显线的场景，在纹理密集的环境中会产生大量事件，但没有明显的线结构；跟踪器容易检测到伪线（spurious lines）。在这种情况下：
    - 事件数量足以让这些伪线通过初始化步骤并持续存在；
    - 稳定的事件流让跟踪器没有理由删除这些线；
    - 休眠机制反而延缓了删除，使“伪线”存在更久。
不过这种情况通常不会出现在电力线巡检任务中。

- 当事件非常稀疏时，跟踪器可能无法跟踪线段。这通常发生在无人机飞行路径与电力线完全平行时。
---

# 文章构想

![事件相机在3D重建的应用](事件相机应用3D重建.png)

《Event-based 3D Reconstruction of Tiny Linear Obstacles for UAV Safety》

- 现有Event-NeRF很强，但在sub-pixel线状结构上表现不足，而安全飞行恰恰对这类结构最敏感
- 我们的目标是在户外/工地等真实场景中，仅用被动事件流 + 机体运动实现3D线状障碍重建，不依赖激光/结构光。

**Q:已有的基于事件相机的SLAM工作中是不是已经把细线这类识别出来并三维重建了？**
**A:来源于GPT对ETH研究现状的检索总结：**
1. **现有event-SLAM大概做到哪一步？**
从ETH这个大综述库里的SLAM和深度估计部分可以看出，目前主流event-SLAM/VO工作主要是：
- 用事件做**角点、边缘特征**，再做VO/SLAM（如SuperEvent等）。
- 用事件做**稀疏/半稠密深度估计、3D重建**，多是针对墙面、桌子、树干等尺度正常的结构。
- 一些工作会输出“线特征”或利用线约束，比如：
    - N-point 线与运动估计（Gao 2024 CVPR，Line and Motion Estimation with Event Cameras）；
    - 实时线段检测与跟踪（Ikura 2025 ICCV-W：Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera）。
这些方法里，**线条只是几何特征之一**：
- 线段通常是门框、桌边、建筑边缘这类“粗线”，像素上宽度比较明显；
- SLAM输出是稀疏点云/线段集合，用来估计位姿和粗地图，**没有专门为了“线状障碍物安全间隙”去做高精度建模**。

1. **已有“线 + 无人机”的工作做到哪儿了？**

ETH 列表里有一个比较接近你关心方向的：
- Asynchronous Event-based Line Tracking for Time-to-Contact Maneuvers in UAS（IROS 2020）。它用事件相机跟踪一条线（比如跑道、导引线），估计time-to-contact来控制无人机机动。

但注意几个区别：
- 它关注的是**视觉伺服/跟踪一条明显的线**，而不是在复杂3D场景里把所有细线障碍物找出来；
- 基本是“2D线 + 距离/时间到碰撞”的控制问题，**不是通用SLAM中的3D线状障碍建图**；
- 场景多是单条线或简单结构，没有“工地里一堆乱七八糟的钢丝/缆绳”。

3. 为什么现有SLAM一般“顺带”也不会把超细线建得很好？

即使算法是edge/line-based，**极细线（电线、钢丝绳）在SLAM里的表现通常还是不好**：
- **触发事件太少/信噪比低**
    - 线太细 → 占的像素少，要么只在极少数像素上触发事件，要么直接淹没在噪声里；
    - 现有SLAM多数会做事件聚合、滤波、下采样，细线很容易被“平滑掉”。
- **标准SLAM追求“整体几何”而不是“最细的结构”**
    - 优先保证大面、刚性物体的几何一致性，细线这类对位姿估计贡献不大，往往不会被特别强调或精细建模；
    - 地图常常是半稠密/稀疏的点云，**不会给你一根连续的3D曲线**，更谈不上安全间隙评估。
- **几乎没有“细线障碍物”的基准数据集与指标**
    - 现有数据集基本是街景、室内、森林等，确实可能包含电线/树枝，但没有谁去标注“哪些是可致命的细线障碍物、重建误差多少、避障成功率多少”；
    - 没有benchmark → 没人会专门针对这个问题优化与claim。